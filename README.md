# Image-Captioning-Model-87-accuracy-
# ğŸ§  Image Captioning Model (87% Accuracy)

This repository contains a fully trained image captioning model based on [`nlpconnect/vit-gpt2-image-captioning`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning). The model was **fine-tuned on a 40,000-image subset of the MS COCO dataset** and achieved an impressive **87% captioning accuracy** on validation data.

## Project Highlights

- âœ… Model: Vision Transformer (ViT) + GPT2
- âœ… Dataset: MS COCO (40k subset)
- âœ… Accuracy: 87%
- âœ… Framework: PyTorch
- âœ… Use Case: Generate meaningful and coherent captions for input images
- âœ… Supports further enhancement via multilingual TTS, hazardous object detection (optional)

---

## Model Architecture Overview

### Vision Transformer (ViT)
- Used for encoding image features.
- Breaks an image into patches and processes them like tokens.
- Trained to learn **visual context representations**.

### GPT-2 Decoder
- Pretrained autoregressive language model.
- Generates natural language captions **word by word** based on encoded image context.
- Learns grammar, structure, and contextual correctness.

### â›“ï¸ Combined Encoder-Decoder Pipeline
- The `nlpconnect/vit-gpt2-image-captioning` model leverages a **VisionEncoderDecoderModel**.
- Image â†’ ViT Encoder â†’ Contextual Embeddings â†’ GPT2 Decoder â†’ Caption

---

## ğŸ“Š Training Details

| Detail                   | Description                          |
|-------------------------|--------------------------------------|
| Dataset                 | MS COCO (subset: 40,000 images)      |
| Epochs                  | 4 (configurable)                     |
| Evaluation Metric       | BLEU / Accuracy                      |
| Final Accuracy Achieved | **87%**                              |
| Loss Function           | CrossEntropy                         |
| Optimizer               | AdamW                                |
| Framework               | PyTorch                              |

---

## ğŸ§ª How to Use

Clone the repo and run inference:

```bash
git clone https://github.com/vishnu2005/Image-Captioning-Model-87-accuracy-.git
cd Image-Captioning-Model-87-accuracy-
python run_inference.py --image path/to/your/image.jpg
```

## ğŸ§± Download Trained Model

Due to GitHub's 100MB file size limit, the trained model file `pytorch_model.bin` (912MB) is hosted externally.

ğŸ‘‰ [Click here to download the model](https://drive.google.com/uc?export=download&id=1plxo7TuJjjTnylXzuyc_h9Ds7pOQyrIx)

After downloading, place the file inside your project directory like this:
final_trained_model/
â”œâ”€â”€ pytorch_model.bin ğŸ‘ˆ put it here
â”œâ”€â”€ run_inference.py
â”œâ”€â”€ caption_generator.py
...

